{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b66bada-2507-4b81-9980-4f5ece07822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "from tensorflow.keras.layers import Conv2D, Add, ReLU, Concatenate, Lambda\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "import glob\n",
    "import random\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cafbd91-71d6-4747-933a-9823c3acc510",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378519df-0ac2-454c-bd8e-4568b7b0f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8\n",
    "batch_size = 10\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4708ab-15cd-4531-b30f-b459261c331b",
   "metadata": {},
   "source": [
    "NETWORK DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b1fe82-cca7-47da-a4a3-23d6c11ebe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haze_net(input_shape=(480, 640, 3), weight_decay=0.0):\n",
    "    X = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(3, (1, 1), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
    "                   kernel_regularizer=l2(weight_decay))(X)\n",
    "\n",
    "    conv2 = Conv2D(3, (3, 3), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
    "                   kernel_regularizer=l2(weight_decay))(conv1)\n",
    "    concat1 = Concatenate(axis=-1)([conv1, conv2])\n",
    "\n",
    "    conv3 = Conv2D(3, (5, 5), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
    "                   kernel_regularizer=l2(weight_decay))(concat1)\n",
    "    concat2 = Concatenate(axis=-1)([conv2, conv3])\n",
    "\n",
    "    conv4 = Conv2D(3, (7, 7), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
    "                   kernel_regularizer=l2(weight_decay))(concat2)\n",
    "    concat3 = Concatenate(axis=-1)([conv1, conv2, conv3, conv4])\n",
    "\n",
    "    conv5 = Conv2D(3, (3, 3), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=RandomNormal(stddev=0.02),\n",
    "                   kernel_regularizer=l2(weight_decay))(concat3)\n",
    "\n",
    "    K = conv5\n",
    "\n",
    "    output = ReLU(max_value=1.0)(K * X - K + 1.0)\n",
    "\n",
    "    model = Model(inputs=X, outputs=output)\n",
    "\n",
    "    trainable_variables = []\n",
    "    for layer in model.layers:\n",
    "        trainable_variables += layer.trainable_variables\n",
    "\n",
    "    return model, trainable_variables\n",
    "    \n",
    "def haze_res_net(X, weight_decay=0.0):\n",
    "    conv1 = Conv2D(3, (1, 1), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=tf.initializers.RandomNormal(),\n",
    "                   kernel_regularizer=l2(weight_decay))(X)\n",
    "\n",
    "    conv2 = Conv2D(3, (3, 3), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=tf.initializers.RandomNormal(),\n",
    "                   kernel_regularizer=l2(weight_decay))(conv1)\n",
    "\n",
    "    add1 = Add()([conv1, conv2])\n",
    "\n",
    "    conv3 = Conv2D(3, (5, 5), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=tf.initializers.RandomNormal(),\n",
    "                   kernel_regularizer=l2(weight_decay))(add1)\n",
    "\n",
    "    conv4 = Conv2D(3, (7, 7), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=tf.initializers.RandomNormal(),\n",
    "                   kernel_regularizer=l2(weight_decay))(conv3)\n",
    "\n",
    "    add2 = Add()([conv3, conv4])\n",
    "\n",
    "    conv5 = Conv2D(3, (3, 3), padding=\"SAME\", activation=\"relu\", use_bias=True,\n",
    "                   kernel_initializer=tf.initializers.RandomNormal(),\n",
    "                   kernel_regularizer=l2(weight_decay))(add2)\n",
    "\n",
    "    add3 = Add()([conv5, conv1])\n",
    "    K = add3\n",
    "\n",
    "    output = ReLU(max_value=1.0)(tf.math.multiply(K, X) - K + 1.0)\n",
    "\n",
    "    model = tf.keras.Model(inputs=X, outputs=output)\n",
    "\n",
    "    trainable_variables = []\n",
    "    for layer in model.layers:\n",
    "        trainable_variables += layer.trainable_variables\n",
    "\n",
    "    return model, trainable_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf80904-e83f-4f17-87a6-759e1f461fa8",
   "metadata": {},
   "source": [
    "DATA LOADING AND PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4a504d-f596-4b8e-b4fa-3d1a0e15ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data_paths(orig_images_path, hazy_images_path):\n",
    "    orig_image_paths = glob.glob(orig_images_path + \"/*.jpg\")\n",
    "    n = len(orig_image_paths)\n",
    "    random.shuffle(orig_image_paths)\n",
    "\n",
    "    train_keys = orig_image_paths[:int(0.90 * n)]\n",
    "    val_keys = orig_image_paths[int(0.90 * n):]\n",
    "\n",
    "    split_dict = {}\n",
    "    for key in train_keys:\n",
    "        split_dict[key] = 'train'\n",
    "    for key in val_keys:\n",
    "        split_dict[key] = 'val'\n",
    "\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "\n",
    "    hazy_image_paths = glob.glob(hazy_images_path + \"/*.jpg\")\n",
    "    for path in hazy_image_paths:\n",
    "        label = os.path.basename(path)\n",
    "        orig_filename = label.split('_')[0] + '_' + label.split('_')[1] + \".jpg\"\n",
    "        orig_path = os.path.join(orig_images_path, orig_filename)\n",
    "        if orig_path in split_dict:\n",
    "            if split_dict[orig_path] == 'train':\n",
    "                train_data.append([path, orig_path])\n",
    "            else:\n",
    "                val_data.append([path, orig_path])\n",
    "        else:\n",
    "            print(f\"Warning: {orig_path} not found in split_dict.\")\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7da733-13d3-4e2b-a597-c8541e6733a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(X):\n",
    "  X = tf.io.read_file(X)\n",
    "  X = tf.image.decode_jpeg(X,channels=3)\n",
    "  X = tf.image.resize(X,(480,640))\n",
    "  X = X / 255.0\n",
    "  return X\n",
    "\n",
    "def showImage(x):\n",
    "  x = np.asarray(x*255,dtype=np.int32)\n",
    "  plt.figure()\n",
    "  plt.imshow(x)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fb094c-385a-49ab-8245-c301921fe076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets(train_data, val_data, batch_size):\n",
    "    train_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in train_data]).map(lambda x: load_image(x))\n",
    "    train_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in train_data]).map(lambda x: load_image(x))\n",
    "    train_ds = tf.data.Dataset.zip((train_ds_hazy, train_ds_orig)).shuffle(100).repeat().batch(batch_size)\n",
    "\n",
    "    val_ds_hazy = tf.data.Dataset.from_tensor_slices([data[0] for data in val_data]).map(lambda x: load_image(x))\n",
    "    val_ds_orig = tf.data.Dataset.from_tensor_slices([data[1] for data in val_data]).map(lambda x: load_image(x))\n",
    "    val_ds = tf.data.Dataset.zip((val_ds_hazy, val_ds_orig)).shuffle(100).repeat().batch(batch_size)\n",
    "\n",
    "    iterator = iter(train_ds)\n",
    "\n",
    "    train_init_op = iterator\n",
    "    val_init_op = iter(val_ds)\n",
    "\n",
    "    return train_init_op, val_init_op, iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affa4895-6f1e-4f35-98b4-e72c430add68",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7a6fd-a018-42aa-923e-9ba920e073fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(9999)\n",
    "\n",
    "train_data, val_data = setup_data_paths(\n",
    "    orig_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/orig_images\",\n",
    "    hazy_images_path=\"./All-In-One-Image-Dehazing-Tensorflow/data/hazy_images\"\n",
    ")\n",
    "\n",
    "train_init_op, val_init_op, iterator = create_datasets(train_data, val_data, batch_size)\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "hazy_image_placeholder = tf.keras.Input(shape=(480, 640, 3), dtype=tf.float32)\n",
    "original_image_placeholder = tf.keras.Input(shape=(480, 640, 3), dtype=tf.float32)\n",
    "\n",
    "model, dehazed_X = haze_net(input_shape=(480, 640, 3))\n",
    "\n",
    "optimizer = Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "trainable_variables = model.trainable_variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79720f0-bdfb-4f76-8367-84b25381ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_steps = 0\n",
    "\n",
    "    for step, (hazy_batch, original_batch) in enumerate(train_init_op):\n",
    "        with tf.GradientTape() as tape:\n",
    "            dehazed_batch = model(hazy_batch, training=True)\n",
    "            current_loss = tf.reduce_mean(tf.square(dehazed_batch - original_batch))\n",
    "\n",
    "        gradients = tape.gradient(current_loss, trainable_variables)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, 0.1)\n",
    "        optimizer.apply_gradients(zip(clipped_gradients, trainable_variables))\n",
    "\n",
    "        epoch_loss += current_loss.numpy()\n",
    "        num_steps += 1\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {step}, Loss: {current_loss.numpy()}\")\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / num_steps\n",
    "        if avg_epoch_loss < 0.1:\n",
    "            print(\"Average epoch loss below 0.1. Training stopped.\")\n",
    "            break\n",
    "\n",
    "    model.save(\"dehazer1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516a6839-6307-4210-a0f4-1db8489dd2b4",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3b10c-1975-4c06-8236-3568fd0fda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iterator = iter(val_init_op)\n",
    "val_steps_per_epoch = len(val_data) // batch_size\n",
    "\n",
    "# Evaluation loop\n",
    "for epoch in range(n_epochs):\n",
    "    total_val_loss = 0.0\n",
    "    for step in range(val_steps_per_epoch):\n",
    "        hazy_batch, original_batch = next(val_iterator)\n",
    "\n",
    "        dehazed_batch = model(hazy_batch, training=False)\n",
    "\n",
    "        val_loss = tf.reduce_mean(tf.square(dehazed_batch - original_batch))\n",
    "        total_val_loss += val_loss.numpy()\n",
    "\n",
    "    avg_val_loss = total_val_loss / val_steps_per_epoch\n",
    "\n",
    "    print(f\"Epoch {epoch}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "model.save(\"dehazer1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833f864-14c4-49aa-93e4-0f3808990998",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
